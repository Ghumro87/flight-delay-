# -*- coding: utf-8 -*-
"""Untitled26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Eb1f8cm-TurmVmhAhpWV2Snpwl8ozgh
"""

!pip install matplotlib

# ======================================================
# CIS 412 - Flight Delay Project (Final Code)
# Logistic Regression vs Decision Tree vs Random Forest
# Includes:
# - Data cleaning
# - Target variable creation
# - One-Hot Encoding + Standardization
# - Train/Test split
# - Model training (3 models)
# - Evaluation (Accuracy, Precision, Recall, F1, Confusion Matrix)
# - 5-fold Cross-Validation
# - ROC Curves and AUC
# - Expected Value / Profit calculation
# - max_depth tuning for Decision Tree
# ======================================================

# ============= 0. Imports =============
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    classification_report,
    confusion_matrix,
    roc_curve,
    roc_auc_score
)

from IPython.display import display

# ============= 1. Load Data =============
# In Google Colab: this will open a file upload dialog.
# You can upload "FlightDelays(2).csv" or "FlightDelays.csv".
try:
    from google.colab import files
    uploaded = files.upload()
    file_name = list(uploaded.keys())[0]
except Exception:
    # Fallback: if not running in Colab, set filename manually
    file_name = "FlightDelays(2).csv"

df = pd.read_csv(file_name)
print("Initial dataset shape:", df.shape)
display(df.head())

print("\nMissing values per column:")
display(df.isnull().sum())

# ============= 2. Data Cleaning & Target Variable =============
# Remove any rows with missing values to avoid issues in training.
df = df.dropna()
print("\nShape after dropping missing values:", df.shape)

# Create a binary target variable:
# 1 = delayed, 0 = on-time
df["Delayed"] = (df["Flight Status"].str.lower() == "delayed").astype(int)

# ============= 3. Feature Selection & Preprocessing =============
# Define numerical and categorical features that will be used as predictors.
numeric_features = [
    "CRS_DEP_TIME",   # scheduled departure time
    "DEP_TIME",       # actual departure time
    "DISTANCE",       # flight distance
    "DAY_WEEK",
    "DAY_OF_MONTH",
    "Weather"         # weather delay indicator (if available)
]

categorical_features = [
    "CARRIER",        # airline carrier
    "ORIGIN",         # origin airport
    "DEST"            # destination airport
]

# Keep only features that actually exist in the dataset
numeric_features = [col for col in numeric_features if col in df.columns]
categorical_features = [col for col in categorical_features if col in df.columns]
feature_cols = numeric_features + categorical_features

X = df[feature_cols]
y = df["Delayed"]

print("\nNumeric features used:", numeric_features)
print("Categorical features used:", categorical_features)

# Preprocessor:
# - Standardize numeric features
# - One-Hot Encode categorical features
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown="ignore")

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
    ]
)

# ============= 4. Train/Test Split =============
# Split into training and test sets (70% train, 30% test).
# Stratify on y to keep the same delay rate in both sets.
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42,
    stratify=y
)

print("\nTrain size:", X_train.shape[0], " | Test size:", X_test.shape[0])

# ============= 5. Define Models =============
# 1) Logistic Regression
log_reg_model = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("model", LogisticRegression(
        max_iter=5000,   # ensure convergence
        solver="lbfgs"
    ))
])

# 2) Decision Tree (baseline version)
tree_model = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("model", DecisionTreeClassifier(
        random_state=42,
        max_depth=6,
        min_samples_leaf=20
    ))
])

# 3) Random Forest
rf_model = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("model", RandomForestClassifier(
        random_state=42,
        n_estimators=200,
        max_depth=None,
        n_jobs=-1
    ))
])

# ============= 6. Train Models =============
log_reg_model.fit(X_train, y_train)
tree_model.fit(X_train, y_train)
rf_model.fit(X_train, y_train)

# ============= 7. Evaluation on Test Set =============
def evaluate_model(name, model, X_test, y_test):
    """
    Evaluate a classification model on the test set using:
    - Accuracy
    - Precision (for Delayed = 1)
    - Recall (for Delayed = 1)
    - F1-score (for Delayed = 1)
    Also prints the confusion matrix and a full classification report.
    Returns predicted probabilities and confusion matrix.
    """
    print("=" * 70)
    print(f"MODEL: {name}")

    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]  # probability of class 1 (Delayed)

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, pos_label=1)
    rec = recall_score(y_test, y_pred, pos_label=1)
    f1 = f1_score(y_test, y_pred, pos_label=1)

    print(f"Accuracy : {acc:.4f}")
    print(f"Precision: {prec:.4f} (Delayed = 1)")
    print(f"Recall   : {rec:.4f} (Delayed = 1)")
    print(f"F1-score : {f1:.4f} (Delayed = 1)")

    cm = confusion_matrix(y_test, y_pred)
    print("\nConfusion Matrix (rows = actual, columns = predicted):")
    print(cm)

    print("\nClassification Report:")
    print(classification_report(
        y_test, y_pred,
        target_names=["on-time (0)", "delayed (1)"]
    ))

    return y_proba, cm

proba_log, cm_log = evaluate_model("Logistic Regression", log_reg_model, X_test, y_test)
proba_tree, cm_tree = evaluate_model("Decision Tree", tree_model, X_test, y_test)
proba_rf, cm_rf = evaluate_model("Random Forest", rf_model, X_test, y_test)

# ============= 8. 5-fold Cross-Validation (Accuracy) =============
def cross_val_accuracy(name, model, X, y, cv=5):
    """
    Compute and print k-fold cross-validation accuracy scores.
    This helps assess model stability and generalization.
    """
    scores = cross_val_score(model, X, y, cv=cv, scoring="accuracy")
    print("=" * 70)
    print(f"{name} - {cv}-fold Cross-Validation Accuracy")
    print("Scores:", np.round(scores, 4))
    print("Mean  :", scores.mean().round(4))
    print("Std   :", scores.std().round(4))

cross_val_accuracy("Logistic Regression", log_reg_model, X, y, cv=5)
cross_val_accuracy("Decision Tree", tree_model, X, y, cv=5)
cross_val_accuracy("Random Forest", rf_model, X, y, cv=5)

# ============= 9. ROC Curves & AUC =============
def plot_roc_curves(y_test, proba_dict):
    """
    Plot ROC curves and show AUC for multiple models on the same figure.
    """
    plt.figure(figsize=(8, 6))
    for name, proba in proba_dict.items():
        fpr, tpr, _ = roc_curve(y_test, proba)
        auc = roc_auc_score(y_test, proba)
        plt.plot(fpr, tpr, label=f"{name} (AUC = {auc:.3f})")

    plt.plot([0, 1], [0, 1], "k--")  # random guess line
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curves - Flight Delay Prediction")
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()

plot_roc_curves(y_test, {
    "Logistic Regression": proba_log,
    "Decision Tree": proba_tree,
    "Random Forest": proba_rf
})

# ============= 10. Expected Value / Profit =============
# Define a simple cost-benefit matrix for business interpretation:
# TP (correctly predicting a delay)   -> +200
# FN (missed delay)                   -> -400
# FP (false alarm delay)              -> -50
# TN (correctly predicting on-time)   -> 0

benefits = {
    "TP": 200,
    "FN": -400,
    "FP": -50,
    "TN": 0
}

def expected_value_from_confusion_matrix(cm, costs):
    """
    Compute expected value per flight using a confusion matrix and a cost-benefit dictionary.
    cm format:
        [[TN, FP],
         [FN, TP]]
    """
    TN, FP, FN, TP = cm.ravel()
    total = TN + FP + FN + TP

    EV = (
        TP * costs["TP"] +
        FN * costs["FN"] +
        FP * costs["FP"] +
        TN * costs["TN"]
    ) / total

    return EV

ev_log = expected_value_from_confusion_matrix(cm_log, benefits)
ev_tree = expected_value_from_confusion_matrix(cm_tree, benefits)
ev_rf = expected_value_from_confusion_matrix(cm_rf, benefits)

print("=" * 70)
print("Expected Profit per Flight (based on the cost-benefit matrix):")
print(f"Logistic Regression: {ev_log:.2f} per flight")
print(f"Decision Tree      : {ev_tree:.2f} per flight")
print(f"Random Forest      : {ev_rf:.2f} per flight")

# ============= 11. Hyperparameter Tuning: max_depth for Decision Tree =============
print("\n=== Hyperparameter Tuning: max_depth for Decision Tree ===")

max_depth_range = list(range(1, 21))
results = {
    "max_depth": [],
    "accuracy": [],
    "f1_score": [],
    "roc_auc": []
}

for depth in max_depth_range:
    tuned_tree = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", DecisionTreeClassifier(
            max_depth=depth,
            random_state=42,
            min_samples_leaf=10  # small regularization to reduce overfitting
        ))
    ])

    tuned_tree.fit(X_train, y_train)
    y_test_pred = tuned_tree.predict(X_test)
    y_test_proba = tuned_tree.predict_proba(X_test)[:, 1]

    acc = accuracy_score(y_test, y_test_pred)
    f1 = f1_score(y_test, y_test_pred, pos_label=1)
    auc = roc_auc_score(y_test, y_test_proba)

    results["max_depth"].append(depth)
    results["accuracy"].append(acc)
    results["f1_score"].append(f1)
    results["roc_auc"].append(auc)

results_df = pd.DataFrame(results)
print("\nFirst rows of max_depth tuning results:")
display(results_df.head())

# Find the best max_depth according to each metric
best_f1_row = results_df.loc[results_df["f1_score"].idxmax()]
best_acc_row = results_df.loc[results_df["accuracy"].idxmax()]
best_auc_row = results_df.loc[results_df["roc_auc"].idxmax()]

print("\nBest max_depth based on F1-score:")
display(best_f1_row)

print("\nBest max_depth based on Accuracy:")
display(best_acc_row)

print("\nBest max_depth based on ROC AUC:")
display(best_auc_row)

# Plot performance vs max_depth
plt.figure(figsize=(10, 6))
plt.plot(results_df["max_depth"], results_df["accuracy"], marker="o", label="Accuracy")
plt.plot(results_df["max_depth"], results_df["f1_score"], marker="o", label="F1-score")
plt.plot(results_df["max_depth"], results_df["roc_auc"], marker="o", label="ROC AUC")
plt.xlabel("max_depth")
plt.ylabel("Score")
plt.title("Decision Tree Performance vs. max_depth")
plt.xticks(results_df["max_depth"])
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()